# Mini-ETL Project: Getting Started with DE Tools

This repository contains my **mini-ETL project** for the Data Engineering course, Task 5: *Getting Started with DE Tools: MinIO, PostgreSQL, and Airflow*. The goal of this project was to set up the three core components of a modern ETL pipeline and run a small workflow to understand how they interact in practice.

---

## ğŸš€ Project Overview

In this project, I built a **complete ETL pipeline** using:

1. **MinIO (Landing Zone)**
   - Stores and retrieves raw CSV data files.
   - Functions as an S3-compatible object storage.

2. **PostgreSQL (Data Warehouse)**
   - Stores structured and transformed data.
   - Allows running SQL queries to validate the ETL process.

3. **Apache Airflow (Orchestration Tool)**
   - Automates and schedules ETL tasks.
   - Orchestrates Extract â†’ Transform â†’ Load (ETL) workflow via DAGs.

---

## ğŸ› ï¸ Tools & Technology Stack

- **Python 3.12**
- **Pandas** for data transformation
- **SQLAlchemy & psycopg2-binary** for PostgreSQL integration
- **MinIO Python SDK** to interact with object storage
- **Docker & Docker Compose** for environment setup
- **Airflow 3** to orchestrate ETL tasks

---

## ğŸ“‚ Project Structure

```
mini-etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_run_dag.py          # Airflow DAG for ETL pipeline
â”œâ”€â”€ plugins/                     # Airflow plugins (if any)
â”œâ”€â”€ logs/                        # Airflow logs
â”œâ”€â”€ employees_large.csv          # Input CSV file
â”œâ”€â”€ employees_summary.csv        # ETL output
â”œâ”€â”€ generate_csv.py              # Script to generate sample CSVs
â”œâ”€â”€ etl.py                       # Main ETL script
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile                   # Custom Airflow Docker image
â”œâ”€â”€ docker-compose.yml           # Docker Compose setup
â”œâ”€â”€ .gitignore                   # Git ignore file
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

```bash
git clone 
cd mini-etl
```

### 2. Build & Start Docker Containers

```bash
docker compose down
docker compose up --build -d
```

### 3. Access Airflow UI

Open in your browser:

```
http://localhost:8080
```

Login credentials (default):

```
Username: admin
Password: admin
```

### 4. Trigger ETL DAG

- DAG Name: `run_etl_docker`
- Tasks: Extract â†’ Transform â†’ Load
- Schedule: Manual trigger

### 5. Verify PostgreSQL

Check the loaded data:

```sql
SELECT * FROM employees_summary;
```

---

## ğŸ§© ETL Workflow

1. **Extract:** Download CSV from MinIO using Python.
2. **Transform:** Clean and summarize data using pandas.
3. **Load:** Insert cleaned data into PostgreSQL.
4. **Orchestrate:** Airflow DAG executes all steps sequentially.


This project forms a solid foundation before moving on to team-based ETL projects and more advanced data engineering pipelines.

---
